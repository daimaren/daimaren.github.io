---
layout:     post
title:      IOS音视频开发（一）如何采集音频
subtitle:   
date:       2020-05-10
author:     Glen
header-img: img/post-bg-none.jpg
catalog: true
tags:
    - IOS
    - 音视频
---

​	iOS平台提供了多套API采集音频，如果开发者想要直接指定一个路径，则可以将录制的音频编码到文件中，可以使用AVAudioRecorder这套API，其优点是简单易用，但是其对于想要实时地在内存中获得录音的数据来说，限制性非常强。对此，iOS平台提供了两个层次的API来协助实现，第一种方式是使用AudioQueue，第二种方式是使用AudioUnit，实际上AudioQueue是AudioUnit更高级的封装，相比较于AudioUnit，它提供的功能更单一，使用的接口调用更简单，具体使用哪个层次的API需要根据应用场景来决定。如果仅仅是要获取内存中的录音数据，然后再进行编码输出（有可能是输出到本地磁盘，也有可能是网络），那么使用更高级的AudioQueue的API会更好一些；如果要使用更多的音效处理，以及实时的监听（在耳机中可以听到自己说的话，在唱歌的App中这是一个最基础的功能），那么使用AudioUnit会更加方便一些。本书的代码案例中，使用的都是AudioUnit，因为本书案例实现的场景不仅需要耳返，还需要音效的实时处理等功能。

​	要想使用iOS的麦克风进行录音，首先要为App声明使用麦克风的权限，在新建的目录下面找到info.plist，然后在其中新增麦克风权限的声明：

```
<key>NSMicrophoneUsageDescription</key>
<string>microphoneDesciption</string>
```

​	这样添加之后，系统就知道了App要访问系统的麦克风权限。下面直接来看如何使用AudioUnit实现人声录制，同时，还会发送给耳机一个监听耳返。

​	要使用AudioUnit，首先需要通过AVAudioSession来开启硬件设备以及对硬件设备做一些设置，然后才能使用AudioUnit，而AVAudioSession的具体使用方法也和前面讲解的一致，下面再来熟悉一下：

- 获得AVAudioSession的实例，由于AVAudioSession是单例模式设计的，所以在这里只需要调用它的单例方法就可以获得。
- 为AudioSession设置使用类别，由于要在录音的同时为用户输送一路监听耳返，所以这里选择使用类别AVAudioSessionCategoryPlayAndRecord，本书前面的章节中，仅在做音频渲染时使用到类别AVAudioSessionCategoryPlayback。所以设置这个类别的目的是告诉系统的硬件应该为我们的App提供什么样的服务。
- 为AudioSession设置预设的采样率。
- 启用AudioSession。
- 为AudioSession设置路由监听器，目的就是在采集音频或者音频输出的线路发生变化的时候（比如插拔耳机、蓝牙设备连接成功等）回调此方法，以便开发者可以重新设置使用当前最新的麦克风或扬声器。

​	至此AudioSession就设置好了，接下来的事情就是构造该应用所使用的AUGraph，其构造步骤与音频渲染构造的AUGraph也很类似，因为这里要使用录音功能，所以需要启用RemoteIO这个AudioUnit的InputElement。RemoteIO这个AudioUnit比较特别，Input-Element实际上使用的是麦克风，而OutputElement使用的则是扬声器，所以这里首先会启用RemoteIOUnit的InputElement。为了支持所开发的App可以在后续Mix一轨伴奏这一扩展功能，在AUGraph中需要增加MultiChannelMixer这个AudioUnit。由于每个AudioUnit的输入输出格式并不相同，所以这里还要使用AudioConvert这个AudioUnit将输入的AudioUnit连接到MixerUnit上。最终将MixerUnit连接到RemoteIO这个AudioUnit的OutputElement，将声音发送到耳机的扬声器中（如果直接发送到手机的扬声器中就会出现啸叫），这样就将AUGraph整体地建立起来了，如下图所示。

![1588489903364](C:\Users\wjy\AppData\Local\Temp\1588489903364.png)

​	此外，这里还需要实现一个功能，就是将录音得到的数据存储为一个文件。若我们想要获取某个AudioUnit的数据，并将其写入文件，那么可以在它后一级的AudioUnit中增加一个回调，然后在回调方法中将该AudioUnit的数据渲染出来并发送给下一级的AudioUnit，同时也可以去写文件，这种方式已经在音频渲染的时候使用过了。这里就为RemoteIO这个AudioUnit的OutputElement增加一个回调，代码如下：

```
AURenderCallbackStruct finalRenderProc;
finalRenderProc.inputProc = &renderCallback;
finalRenderProc.inputProcRefCon = (__bridge void *)self;
status = AUGraphSetNodeInputCallback(_auGraph, _ioNode, 0, &finalRenderProc);
```

然后在上述回调方法的实现中，将它的前一级MixerUnit的数据渲染出来，同时写文件，代码如下：

```
static OSStatus renderCallback(void *inRefCon, AudioUnitRenderActionFlags *ioActionFlags, const AudioTimeStamp *inTimeStamp, UInt32 inBusNumber, UInt32 inNumberFrames, AudioBufferList *ioData)
{
    OSStatus result = noErr;
    __unsafe_unretained AudioRecorder *THIS = (__bridge AudioRecorder *)inRefCon;
    AudioUnitRender(THIS->_mixerUnit, ioActionFlags, inTimeStamp, 0, inNumberFrames, ioData);
    result = ExtAudioFileWriteAsync(THIS->finalAudioFile, inNumberFrames, ioData);
    return result;
}
```

使用ExtAudioFile来写文件，即, iOS提供的这个API只需要设置好输入格式、输出格式以及输出文件路径和文件格式即可，代码如下：

```
AudioStreamBasicDescription destinationFormat;
CFURLRef destinationURL;
result = ExtAudioFileCreateWithURL(destinationURL,
   kAudioFileCAFType,
   &destinationFormat,
   NULL,
   kAudioFileFlags_EraseFile,
   &finalAudioFile);
// set the audio data format of mixer Unit
result = ExtAudioFileSetProperty(finalAudioFile,
    kExtAudioFileProperty_ClientDataFormat,
    sizeof(clientFormat),
    &clientFormat);
// specify codec
UInt32 codec = kAppleHardwareAudioCodecManufacturer;
result = ExtAudioFileSetProperty(finalAudioFile,
    kExtAudioFileProperty_CodecManufacturer,
    sizeof(codec),
    &codec);
```

在需要编码文件时直接写入数据：

```
result = ExtAudioFileWriteAsync(THIS->finalAudioFile, inNumberFrames, ioData);
```

在停止写入的时候调用关闭方法即可：

```
ExtAudioFileDispose(finalAudioFile);
```

最终就可以得到我们想要的文件了，大家可以从应用的沙盒中将保存的文件读取出来（在XCode中利用Device取出文件或者使用iExplorer等软件取出），然后播放试听一下。本节的代码示例是代码仓库中的AudioRecorder，建议读者运行该项目的时候插上耳机，以免设备发出啸叫的声音，另外，可点击recorder开始录音，点击stop结束录音，并且可取出对应的录音文件在PC上用系统播放器进行播放试听。

