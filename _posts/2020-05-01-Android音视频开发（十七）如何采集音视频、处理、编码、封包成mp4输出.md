---
layout:     post
title:      Android音视频开发（十七）如何采集音视频、处理、编码、封包成mp4输出
subtitle:   
date:       2020-05-01
author:     Glen
header-img: img/post-bg-none.jpg
catalog: true
tags:
    - Android
    - 音视频
    - 录制
---

## 相机预览

**1. 创建EGL环境**

```
if (sharedContext == null) {
    sharedContext = EGL14.EGL_NO_CONTEXT;
}

mEGLDisplay = EGL14.eglGetDisplay(EGL14.EGL_DEFAULT_DISPLAY);
if (mEGLDisplay == EGL14.EGL_NO_DISPLAY) {
    throw new RuntimeException("unable to get EGL14 display");
}
int[] version = new int[2];
if (!EGL14.eglInitialize(mEGLDisplay, version, 0, version, 1)) {
    mEGLDisplay = null;
    throw new RuntimeException("unable to initialize EGL14");
}

// Try to get a GLES3 context, if requested.
if ((flags & FLAG_TRY_GLES3) != 0) {
    //Log.d(TAG, "Trying GLES 3");
    EGLConfig config = getConfig(flags, 3);
    if (config != null) {
        int[] attrib3_list = {
                EGL14.EGL_CONTEXT_CLIENT_VERSION, 3,
                EGL14.EGL_NONE
        };
        EGLContext context = EGL14.eglCreateContext(mEGLDisplay, config, sharedContext,
                attrib3_list, 0);

        if (EGL14.eglGetError() == EGL14.EGL_SUCCESS) {
            //Log.d(TAG, "Got GLES 3 config");
            mEGLConfig = config;
            mEGLContext = context;
            mGlVersion = 3;
        }
    }
}
if (mEGLContext == EGL14.EGL_NO_CONTEXT) {  // GLES 2 only, or GLES 3 attempt failed
    //Log.d(TAG, "Trying GLES 2");
    EGLConfig config = getConfig(flags, 2);
    if (config == null) {
        throw new RuntimeException("Unable to find a suitable EGLConfig");
    }
    int[] attrib2_list = {
            EGL14.EGL_CONTEXT_CLIENT_VERSION, 2,
            EGL14.EGL_NONE
    };
    EGLContext context = EGL14.eglCreateContext(mEGLDisplay, config, sharedContext,
            attrib2_list, 0);
    checkEglError("eglCreateContext");
    mEGLConfig = config;
    mEGLContext = context;
    mGlVersion = 2;
}

// Confirm with query.
int[] values = new int[1];
EGL14.eglQueryContext(mEGLDisplay, mEGLContext, EGL14.EGL_CONTEXT_CLIENT_VERSION,
        values, 0);
Log.d(TAG, "EGLContext created, client version " + values[0]);
```

**2.创建预览Surface**

```
// Create a window surface, and attach it to the Surface we received from surfaceCreated.
int[] surfaceAttribs = {
        EGL14.EGL_NONE
};
EGLSurface mEGLSurface = EGL14.eglCreateWindowSurface(mEGLDisplay, mEGLConfig, surface,
        surfaceAttribs, 0);
checkEglError("eglCreateWindowSurface");
if (eglSurface == null) {
    throw new RuntimeException("surface was null");
}
if (mEGLDisplay == EGL14.EGL_NO_DISPLAY) {
    // called makeCurrent() before create?
    Log.d(TAG, "NOTE: makeCurrent w/o display");
}
if (!EGL14.eglMakeCurrent(mEGLDisplay, mEGLSurface, mEGLSurface, mEGLContext)) {
    throw new RuntimeException("eglMakeCurrent failed");
}
```
**3. 初始化滤镜**

```
// 相机输入滤镜
mFilterArrays.put(RenderIndex.CameraIndex, new GLImageOESInputFilter(context));
// 美颜滤镜
mFilterArrays.put(RenderIndex.BeautyIndex, new GLImageBeautyFilter(context));
// 彩妆滤镜
mFilterArrays.put(RenderIndex.MakeupIndex, new GLImageMakeupFilter(context, null));
// 美型滤镜
mFilterArrays.put(RenderIndex.FaceAdjustIndex, new GLImageFaceReshapeFilter(context));
// LUT/颜色滤镜
mFilterArrays.put(RenderIndex.FilterIndex, null);
// 贴纸资源滤镜
mFilterArrays.put(RenderIndex.ResourceIndex, null);
// 景深滤镜
mFilterArrays.put(RenderIndex.DepthBlurIndex, new GLImageDepthBlurFilter(context));
// 暗角滤镜
mFilterArrays.put(RenderIndex.VignetteIndex, new GLImageVignetteFilter(context));
// 显示输出
mFilterArrays.put(RenderIndex.DisplayIndex, new GLImageFilter(context));
// 人脸关键点调试
mFilterArrays.put(RenderIndex.FacePointIndex, new GLImageFacePointsFilter(context));
```
**4. 滤镜初始化程序句柄**

```
mProgramHandle = OpenGLUtils.createProgram(mVertexShader, mFragmentShader);
mPositionHandle = GLES30.glGetAttribLocation(mProgramHandle, "aPosition");
mTextureCoordinateHandle = GLES30.glGetAttribLocation(mProgramHandle, "aTextureCoord");
mInputTextureHandle = GLES30.glGetUniformLocation(mProgramHandle, "inputTexture");
mIsInitialized = true;
```

**5. 创建外部纹理**

```
mInputTexture = createTexture(GLES11Ext.GL_TEXTURE_EXTERNAL_OES)；
public static int createTexture(int textureType) {
    int[] textures = new int[1];
    GLES30.glGenTextures(1, textures, 0);
    OpenGLUtils.checkGlError("glGenTextures");
    int textureId = textures[0];
    GLES30.glBindTexture(textureType, textureId);
    OpenGLUtils.checkGlError("glBindTexture " + textureId);
    GLES30.glTexParameterf(textureType, GLES30.GL_TEXTURE_MIN_FILTER, GLES30.GL_NEAREST);
    GLES30.glTexParameterf(textureType, GLES30.GL_TEXTURE_MAG_FILTER, GLES30.GL_LINEAR);
    GLES30.glTexParameterf(textureType, GLES30.GL_TEXTURE_WRAP_S, GLES30.GL_CLAMP_TO_EDGE);
    GLES30.glTexParameterf(textureType, GLES30.GL_TEXTURE_WRAP_T, GLES30.GL_CLAMP_TO_EDGE);
    OpenGLUtils.checkGlError("glTexParameter");
    return textureId;
}
```

**6. 创建SurfaceTexture**

```
mSurfaceTexture = new SurfaceTexture(mInputTexture);
```

**7. 初始化相机**

```
mCamera = Camera.open(cameraID);
if (mCamera == null) {
    throw new RuntimeException("Unable to open camera");
}
CameraParam cameraParam = CameraParam.getInstance();
cameraParam.cameraId = cameraID;
Camera.Parameters parameters = mCamera.getParameters();
cameraParam.supportFlash = checkSupportFlashLight(parameters);
cameraParam.previewFps = chooseFixedPreviewFps(parameters, expectFps * 1000);
parameters.setRecordingHint(true);
mCamera.setParameters(parameters);
setPreviewSize(mCamera, expectWidth, expectHeight);
setPictureSize(mCamera, expectWidth, expectHeight);
calculateCameraPreviewOrientation((Activity) context);
mCamera.setDisplayOrientation(cameraParam.orientation);

mCamera.setPreviewTexture(mSurfaceTexture);
mCamera.setPreviewCallback(this);//添加预览回调
```

**8. 启动预览**

```
mCamera.startPreview();
isPreviewing = true;
```

**9. 回调相机数据**

```
public void onPreviewFrame(byte[] data, Camera camera) {
	//绘制帧
	drawFrame()；
}
```

**10. 绘制帧**

```
if (!EGL14.eglMakeCurrent(mEGLDisplay, mEGLSurface, mEGLSurface, mEGLContext)) {
    throw new RuntimeException("eglMakeCurrent failed");
}
mSurfaceTexture.updateTexImage();
mSurfaceTexture.getTransformMatrix(mMatrix);
//滤镜处理
mCurrentTexture = drawFrame(mInputTexture, mMatrix);
EGL14.eglSwapBuffers(mEGLDisplay, mEGLSurface);

// 是否处于录制状态
if (isRecording && !isRecordingPause) {
	//todo<<<<<<<<<<<<<<<<<<<<<<<<
}
```

**11. 多级滤镜处理**

```
public int drawFrame(int inputTexture, float[] mMatrix) {
    int currentTexture = inputTexture;
    if (mFilterArrays.get(RenderIndex.CameraIndex) == null
            || mFilterArrays.get(RenderIndex.DisplayIndex) == null) {
        return currentTexture;
    }
    if (mFilterArrays.get(RenderIndex.CameraIndex) instanceof GLImageOESInputFilter) {
        ((GLImageOESInputFilter)mFilterArrays.get(RenderIndex.CameraIndex)).setTextureTransformMatrix(mMatrix);
    }
    currentTexture = mFilterArrays.get(RenderIndex.CameraIndex)
            .drawFrameBuffer(currentTexture, mVertexBuffer, mTextureBuffer);
    // 如果处于对比状态，不做处理
    if (!mCameraParam.showCompare) {
        // 美颜滤镜
        if (mFilterArrays.get(RenderIndex.BeautyIndex) != null) {
            if (mFilterArrays.get(RenderIndex.BeautyIndex) instanceof IBeautify
                    && mCameraParam.beauty != null) {
                ((IBeautify) mFilterArrays.get(RenderIndex.BeautyIndex)).onBeauty(mCameraParam.beauty);
            }
            currentTexture = mFilterArrays.get(RenderIndex.BeautyIndex).drawFrameBuffer(currentTexture, mVertexBuffer, mTextureBuffer);
        }

        // 彩妆滤镜
        if (mFilterArrays.get(RenderIndex.MakeupIndex) != null) {
            currentTexture = mFilterArrays.get(RenderIndex.MakeupIndex).drawFrameBuffer(currentTexture, mVertexBuffer, mTextureBuffer);
        }

        // 美型滤镜
        if (mFilterArrays.get(RenderIndex.FaceAdjustIndex) != null) {
            if (mFilterArrays.get(RenderIndex.FaceAdjustIndex) instanceof IBeautify) {
                ((IBeautify) mFilterArrays.get(RenderIndex.FaceAdjustIndex)).onBeauty(mCameraParam.beauty);
            }
            currentTexture = mFilterArrays.get(RenderIndex.FaceAdjustIndex).drawFrameBuffer(currentTexture, mVertexBuffer, mTextureBuffer);
        }

        // 绘制颜色滤镜
        if (mFilterArrays.get(RenderIndex.FilterIndex) != null) {
            currentTexture = mFilterArrays.get(RenderIndex.FilterIndex).drawFrameBuffer(currentTexture, mVertexBuffer, mTextureBuffer);
        }

        // 资源滤镜，可以是贴纸、滤镜甚至是彩妆类型
        if (mFilterArrays.get(RenderIndex.ResourceIndex) != null) {
            currentTexture = mFilterArrays.get(RenderIndex.ResourceIndex).drawFrameBuffer(currentTexture, mVertexBuffer, mTextureBuffer);
        }

        // 景深
        if (mFilterArrays.get(RenderIndex.DepthBlurIndex) != null) {
            mFilterArrays.get(RenderIndex.DepthBlurIndex).setFilterEnable(mCameraParam.enableDepthBlur);
            currentTexture = mFilterArrays.get(RenderIndex.DepthBlurIndex).drawFrameBuffer(currentTexture, mVertexBuffer, mTextureBuffer);
        }

        // 暗角
        if (mFilterArrays.get(RenderIndex.VignetteIndex) != null) {
            mFilterArrays.get(RenderIndex.VignetteIndex).setFilterEnable(mCameraParam.enableVignette);
            currentTexture = mFilterArrays.get(RenderIndex.VignetteIndex).drawFrameBuffer(currentTexture, mVertexBuffer, mTextureBuffer);
        }
    }

    // 显示输出，需要调整视口大小
    mFilterArrays.get(RenderIndex.DisplayIndex).drawFrame(currentTexture, mDisplayVertexBuffer, mDisplayTextureBuffer);

    return currentTexture;
}
```

**12. OpenGL渲染**

```
// 没有初始化、输入纹理不合法、滤镜不可用时直接返回
if (!mIsInitialized || textureId == OpenGLUtils.GL_NOT_INIT || !mFilterEnable) {
    return false;
}

// 设置视口大小
GLES30.glViewport(0, 0, mDisplayWidth, mDisplayHeight);
GLES30.glClearColor(0.0f, 0.0f, 0.0f, 1.0f);
GLES30.glClear(GLES30.GL_COLOR_BUFFER_BIT);

// 使用当前的program
GLES30.glUseProgram(mProgramHandle);
// 运行延时任务
runPendingOnDrawTasks();

// 绘制纹理
onDrawTexture(textureId, vertexBuffer, textureBuffer);
```

##  录制

**1. 初始化复用器**

```
MediaMuxer mMediaMuxer = new MediaMuxer(ext, MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4);
```

**2. 初始化音频编码器**

```
final MediaFormat audioFormat = MediaFormat.createAudioFormat(MIME_TYPE, SAMPLE_RATE, 1);
audioFormat.setInteger(MediaFormat.KEY_AAC_PROFILE, MediaCodecInfo.CodecProfileLevel.AACObjectLC);
audioFormat.setInteger(MediaFormat.KEY_CHANNEL_MASK, AudioFormat.CHANNEL_IN_MONO);
audioFormat.setInteger(MediaFormat.KEY_BIT_RATE, BIT_RATE);
audioFormat.setInteger(MediaFormat.KEY_CHANNEL_COUNT, 1);
if (DEBUG) Log.i(TAG, "format: " + audioFormat);

mMediaCodec = MediaCodec.createEncoderByType(MIME_TYPE);
mMediaCodec.configure(audioFormat, null, null, MediaCodec.CONFIGURE_FLAG_ENCODE);
mMediaCodec.start();
```

**3. 初始化视频编码器**

```
final MediaFormat format = MediaFormat.createVideoFormat(MIME_TYPE, videoWidth, videoHeight);
format.setInteger(MediaFormat.KEY_COLOR_FORMAT, MediaCodecInfo.CodecCapabilities.COLOR_FormatSurface);    // API >= 18
if (mBitRate > 0) {
    format.setInteger(MediaFormat.KEY_BIT_RATE, mBitRate);
} else {
    format.setInteger(MediaFormat.KEY_BIT_RATE, calcBitRate());
}
format.setInteger(MediaFormat.KEY_FRAME_RATE, FRAME_RATE);
format.setInteger(MediaFormat.KEY_I_FRAME_INTERVAL, I_FRAME_INTERVAL);
if (DEBUG) Log.i(TAG, "format: " + format);

mMediaCodec = MediaCodec.createEncoderByType(MIME_TYPE);
mMediaCodec.configure(format, null, null, MediaCodec.CONFIGURE_FLAG_ENCODE);
// get Surface for encoder input
// this method only can call between #configure and #start
mSurface = mMediaCodec.createInputSurface();    // API >= 18
mMediaCodec.start();
```

**4. 创建录制EGL环境**

```
//使用预览的sharedContext
if (sharedContext == null) {
    sharedContext = EGL14.EGL_NO_CONTEXT;
}

mEGLDisplay = EGL14.eglGetDisplay(EGL14.EGL_DEFAULT_DISPLAY);
if (mEGLDisplay == EGL14.EGL_NO_DISPLAY) {
    throw new RuntimeException("unable to get EGL14 display");
}
int[] version = new int[2];
if (!EGL14.eglInitialize(mEGLDisplay, version, 0, version, 1)) {
    mEGLDisplay = null;
    throw new RuntimeException("unable to initialize EGL14");
}

// Try to get a GLES3 context, if requested.
if ((flags & FLAG_TRY_GLES3) != 0) {
    //Log.d(TAG, "Trying GLES 3");
    EGLConfig config = getConfig(flags, 3);
    if (config != null) {
        int[] attrib3_list = {
                EGL14.EGL_CONTEXT_CLIENT_VERSION, 3,
                EGL14.EGL_NONE
        };
        EGLContext context = EGL14.eglCreateContext(mEGLDisplay, config, sharedContext,
                attrib3_list, 0);

        if (EGL14.eglGetError() == EGL14.EGL_SUCCESS) {
            //Log.d(TAG, "Got GLES 3 config");
            mEGLConfig = config;
            mEGLContext = context;
            mGlVersion = 3;
        }
    }
}
if (mEGLContext == EGL14.EGL_NO_CONTEXT) {  // GLES 2 only, or GLES 3 attempt failed
    //Log.d(TAG, "Trying GLES 2");
    EGLConfig config = getConfig(flags, 2);
    if (config == null) {
        throw new RuntimeException("Unable to find a suitable EGLConfig");
    }
    int[] attrib2_list = {
            EGL14.EGL_CONTEXT_CLIENT_VERSION, 2,
            EGL14.EGL_NONE
    };
    EGLContext context = EGL14.eglCreateContext(mEGLDisplay, config, sharedContext,
            attrib2_list, 0);
    checkEglError("eglCreateContext");
    mEGLConfig = config;
    mEGLContext = context;
    mGlVersion = 2;
}

// Confirm with query.
int[] values = new int[1];
EGL14.eglQueryContext(mEGLDisplay, mEGLContext, EGL14.EGL_CONTEXT_CLIENT_VERSION,
        values, 0);
Log.d(TAG, "EGLContext created, client version " + values[0]);
```

**5. 创建录制用的EGL Surface**

```
int[] surfaceAttribs = {
        EGL14.EGL_NONE
};
EGLSurface mEGLSurface = EGL14.eglCreateWindowSurface(mEGLDisplay, mEGLConfig, mVideoEncoder.getInputSurface(),
        surfaceAttribs, 0);
checkEglError("eglCreateWindowSurface");
if (!EGL14.eglMakeCurrent(mEGLDisplay, mEGLSurface, mEGLSurface, mEGLContext)) {
    throw new RuntimeException("eglMakeCurrent failed");
}
```

**6. 创建录制用的Filter**

```
if (mRecordFilter == null) {
    mRecordFilter = new GLImageFilter(context);
}
mRecordFilter.onInputSizeChanged(mTextureWidth, mTextureHeight);
mRecordFilter.onDisplaySizeChanged(mVideoWidth, mVideoHeight);
```

**7. 启动音频录制编码线程**

```
if (mAudioThread == null) {
    mAudioThread = new AudioThread();
    mAudioThread.start();
}
```

**8. 音频采集**

```
final int min_buffer_size = AudioRecord.getMinBufferSize(
        SAMPLE_RATE, AudioFormat.CHANNEL_IN_MONO,
        AudioFormat.ENCODING_PCM_16BIT);
int buffer_size = SAMPLES_PER_FRAME * FRAMES_PER_BUFFER;
if (buffer_size < min_buffer_size)
    buffer_size = ((min_buffer_size / SAMPLES_PER_FRAME) + 1) * SAMPLES_PER_FRAME * 2;

AudioRecord audioRecord = null;
for (final int source : AUDIO_SOURCES) {
    try {
        audioRecord = new AudioRecord(
            source, SAMPLE_RATE,
            AudioFormat.CHANNEL_IN_MONO, AudioFormat.ENCODING_PCM_16BIT, buffer_size);
        if (audioRecord.getState() != AudioRecord.STATE_INITIALIZED)
            audioRecord = null;
    } catch (final Exception e) {
        audioRecord = null;
    }
    if (audioRecord != null) break;
}
if (audioRecord != null) {
    mWeakRecorder = new WeakReference<AudioRecord>(audioRecord);
    try {
        if (mIsCapturing) {
            if (DEBUG) {
                Log.d(TAG, "AudioThread:start audio recording");
            }
            final ByteBuffer buf = ByteBuffer.allocateDirect(SAMPLES_PER_FRAME);
            int readBytes;
            synchronized (mSync) {
                audioRecord.startRecording();
                mAudioStarted = true;
            }
            try {
                for (; mIsCapturing && !mRequestStop && !mIsEOS && mAudioStarted;) {
                    // read audio data from internal mic
                    if(isPause){
                        continue;
                    }
                    buf.clear();
                    readBytes = audioRecord.read(buf, SAMPLES_PER_FRAME);
                    if (readBytes > 0 ) {
                        // set audio data to encoder
                        buf.position(readBytes);
                        buf.flip();
                        encode(buf, readBytes, getPTSUs());
                        frameAvailableSoon();
                    }
                }
                frameAvailableSoon();
            } finally {
                synchronized (mSync) {
                    mAudioStarted = false;
                    audioRecord.stop();
                }
            }
        }
    } finally {
        synchronized (mSync) {
            mAudioStarted = false;
            audioRecord.release();
            if (mWeakRecorder != null) {
                mWeakRecorder.clear();
                mWeakRecorder = null;
            }
        }
    }
} else {
    Log.e(TAG, "failed to initialize AudioRecord");
}
```

**9. 音频送编码器**

```
final ByteBuffer[] inputBuffers = mMediaCodec.getInputBuffers();
while (mIsCapturing) {
    final int inputBufferIndex = mMediaCodec.dequeueInputBuffer(TIMEOUT_USEC);
    if (inputBufferIndex >= 0) {
        final ByteBuffer inputBuffer = inputBuffers[inputBufferIndex];
        inputBuffer.clear();
        if (buffer != null) {
            inputBuffer.put(buffer);
        }
        if (length <= 0) {
            // send EOS
            mIsEOS = true;
            if (DEBUG) {
                Log.i(TAG, "send BUFFER_FLAG_END_OF_STREAM");
            }
            mMediaCodec.queueInputBuffer(inputBufferIndex, 0, 0,
                    presentationTimeUs, MediaCodec.BUFFER_FLAG_END_OF_STREAM);
            break;
        } else {
            mMediaCodec.queueInputBuffer(inputBufferIndex, 0, length,
                    presentationTimeUs, 0);
        }
        break;
    } else if (inputBufferIndex == MediaCodec.INFO_TRY_AGAIN_LATER) {
        // wait for MediaCodec encoder is ready to encode
        // nothing to do here because MediaCodec#dequeueInputBuffer(TIMEOUT_USEC)
        // will wait for maximum TIMEOUT_USEC(10msec) on each call
    }
}
```



**消耗编码数据并写入复用器**

```
if (mMediaCodec == null) return;
ByteBuffer[] encoderOutputBuffers = mMediaCodec.getOutputBuffers();
int encoderStatus, count = 0;
final MediaMuxerWrapper muxer = mWeakMuxer.get();
if (muxer == null) {
    Log.w(TAG, "muxer is unexpectedly null");
    return;
}
LOOP:
while (mIsCapturing) {
    // get encoded data with maximum timeout duration of TIMEOUT_USEC(=10[msec])
    encoderStatus = mMediaCodec.dequeueOutputBuffer(mBufferInfo, TIMEOUT_USEC);

    if (encoderStatus == MediaCodec.INFO_TRY_AGAIN_LATER) {
        // wait 5 counts(=TIMEOUT_USEC x 5 = 50msec) until data/EOS come
        if (!mIsEOS) {
            if (++count > 5)
                break LOOP;        // out of while
        }
    } else if (encoderStatus == MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED) {
        if (DEBUG) {
            Log.d(TAG, "INFO_OUTPUT_BUFFERS_CHANGED");
        }
        // this shoud not come when encoding
        encoderOutputBuffers = mMediaCodec.getOutputBuffers();
    } else if (encoderStatus == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) {
        if (DEBUG) {
            Log.d(TAG, "INFO_OUTPUT_FORMAT_CHANGED");
        }
        // this status indicate the output format of codec is changed
        // this should come only once before actual encoded data
        // but this status never come on Android4.3 or less
        // and in that case, you should treat when MediaCodec.BUFFER_FLAG_CODEC_CONFIG come.
        if (mMuxerStarted) {    // second time request is error
            throw new RuntimeException("format changed twice");
        }
        // get output format from codec and pass them to muxer
        // getOutputFormat should be called after INFO_OUTPUT_FORMAT_CHANGED otherwise crash.
        final MediaFormat format = mMediaCodec.getOutputFormat(); // API >= 16

        if (mbIsVideo) {
            Log.d("www", "MediaCodec format: " + format.toString());
        }

        mTrackIndex = muxer.addTrack(format);
        mMuxerStarted = true;
        if (!muxer.start()) {
            // we should wait until muxer is ready
            // 等待复用器准备完成
            synchronized (muxer) {
                while (!muxer.isStarted())
                    try {
                        muxer.wait(100);
                    } catch (final InterruptedException e) {
                        break LOOP;
                    }
            }
        }
    } else if (encoderStatus < 0) {
        // unexpected status
        if (DEBUG) {
            Log.w(TAG, "drain:unexpected result from encoder#dequeueOutputBuffer: "
                    + encoderStatus);
        }
    } else {
        final ByteBuffer encodedData = encoderOutputBuffers[encoderStatus];
        if (encodedData == null) {
            // this never should come...may be a MediaCodec internal error
            throw new RuntimeException("encoderOutputBuffer " + encoderStatus + " was null");
        }
        if ((mBufferInfo.flags & MediaCodec.BUFFER_FLAG_CODEC_CONFIG) != 0) {
            // You shoud set output format to muxer here when you target Android4.3 or less
            // but MediaCodec#getOutputFormat can not call here(because INFO_OUTPUT_FORMAT_CHANGED don't come yet)
            // therefor we should expand and prepare output format from buffer data.
            // This sample is for API>=18(>=Android 4.3), just ignore this flag here
            if (DEBUG) {
                Log.d(TAG, "drain:BUFFER_FLAG_CODEC_CONFIG");
            }
            mBufferInfo.size = 0;
        }

        if (mBufferInfo.size != 0) {
            // encoded data is ready, clear waiting counter
            count = 0;
            if (!mMuxerStarted) {
                // muxer is not ready...this will prrograming failure.
                throw new RuntimeException("drain:muxer hasn't started");
            }

            // write encoded data to muxer(need to adjust presentationTimeUs.
            mBufferInfo.presentationTimeUs = getPTSUs();
            muxer.writeSampleData(mTrackIndex, encodedData, mBufferInfo);
            prevOutputPTSUs = mBufferInfo.presentationTimeUs;
        }
        // return buffer to encoder
        mMediaCodec.releaseOutputBuffer(encoderStatus, false);
        if ((mBufferInfo.flags & MediaCodec.BUFFER_FLAG_END_OF_STREAM) != 0) {
            // when EOS come.
            mIsCapturing = false;
            break;      // out of while
        }
    }
}
```





##  释放资源

**释放滤镜**

```
for (int i = 0; i < RenderIndex.NumberIndex; i++) {
    if (mFilterArrays.get(i) != null) {
        mFilterArrays.get(i).release();
    }
}
mFilterArrays.clear();
```

**释放相机**

```
isPreviewing = false;
if (mCamera != null) {
    mCamera.setPreviewCallback(null);
    mCamera.setPreviewCallbackWithBuffer(null);
    mCamera.addCallbackBuffer(null);
    mCamera.stopPreview();
    mCamera.release();
    mCamera = null;
}
```
**释放SurfaceTexture**

```
if (mSurfaceTexture != null) {
    mSurfaceTexture.release();
    mSurfaceTexture = null;
}
```

**释放EGL资源**

```
EGL14.eglDestroySurface(mEGLDisplay, mEGLSurface);
mEGLSurface = EGL14.EGL_NO_SURFACE;

if (mEGLDisplay != EGL14.EGL_NO_DISPLAY) {
    // Android is unusual in that it uses a reference-counted EGLDisplay.  So for
    // every eglInitialize() we need an eglTerminate().
    EGL14.eglMakeCurrent(mEGLDisplay, EGL14.EGL_NO_SURFACE, EGL14.EGL_NO_SURFACE,
            EGL14.EGL_NO_CONTEXT);
    EGL14.eglDestroyContext(mEGLDisplay, mEGLContext);
    EGL14.eglReleaseThread();
    EGL14.eglTerminate(mEGLDisplay);
}

mEGLDisplay = EGL14.EGL_NO_DISPLAY;
mEGLContext = EGL14.EGL_NO_CONTEXT;
mEGLConfig = null;
```