---
layout:     post
title:      Android视频播放器开发
subtitle:   
date:       2020-01-02
author:     Glen
header-img: img/post-bg-none.jpg
catalog: true
tags:
    - Android
    - 视频播放器
---

### 1. 架构设计

首先来看一下播放器需要为用户提供哪些功能：能够从零开始播放（当然要保证音画对齐）；支持暂停和继续播放功能；支持seek功能（就是可以随意拖动到任意位置并仍然可以继续播放），有的播放器还支持快进、快退15s。下面先来实现最基本的功能，即播放器能够从零开始播放、暂停和继续。首先来思考一下要实现的场景，播放器可以从零开始播放直到结束。如果直接抛出这样一个项目，我们很容易找不到任何头绪，但是作为一个开发人员，要做的事情就是把复杂的问题简单化，简单的问题条理化，最终按照拆分得非常细的模块来逐个实现。基于这个项目，我们需要思考以下几个问题：

- 输入是什么？
- 输出是什么？
- 可以划分为几个模块？
- 每个模块的职责是什么？

​	下面对问题逐个进行梳理，首先要搞清楚输入是什么，输入既可以是本地磁盘上的一个媒体文件（可能是FLV、MP4、AVI、MOV等格式的文件），也可以是网络上的一个媒体文件（可能是HTTP、RTMP、HLS等协议），这就是我们确定的输入；那么输出又是什么呢？输出就是让扬声器播放视频中的音频使用户的耳朵可以听到声音，让屏幕显示视频画面使用户的眼睛可以看到画面，同时，听到的声音和看到的画面必须是同步的（也就是说不能让用户听到的是“你好”的发音，看到的却是“吃了”的画面）；然后再根据输入和输出拆分模块，并为模块分配合理的职责。

​	对于输入部分的分析具体如下，输入有可能是不同的协议，比如说f ile（本地磁盘的文件），或者是HTTP、RTMP、HLS协议等；也有可能是不同的封装格式，比如说MP4、FLV、MOV等封装格式；而对于这些封装格式里面的内容，会有两路流，分别是音频流和视频流，我们需要将这两路流都解码为裸数据。待视频流和音频流都解码为裸数据之后，需要为音视频各自建立一个队列将裸数据存储起来，不过，如果是在需要播放一帧的时候再去做解码，那么这一帧的视频就有可能产生卡顿或者延迟，所以这里引出了第一个线程，即为播放器的后台解码分配一个线程，该线程用于解析协议，处理解封装以及解码，并最终将裸数据放到音频和视频的队列中，这个模块称为输入模块。

​	下面再来看输出部分，输出部分其实是由两部分组成的：一部分是音频的输出，另一部分是视频的输出。不过可以确定的是，不论是音频的输出还是视频的输出，它们都会用一个线程来进行管理，这两个模块应该先从队列中获取音视频的裸数据，然后分别进行音视频的渲染，并最终发布到扬声器和屏幕上，使得用户可以听得到、看得到，这两个模块称为音频输出和视频输出模块。

​	下面再来思考一件事情，由于输出模块都在各自的线程中，音频和视频均是单独播放，这就导致了两个输出模块的播放频率以及线程控制没有任何关系，从而无法保证音画对齐。我们规划的各个模块里，好像还没有一个模块的职责是负责音视频同步的，所以需要再建立一个模块来负责相关的工作，这个模块称为音视频同步模块。至此，模块都已拆分完毕，具体的模块分布如下图所示。

![1588489035926](C:\Users\wjy\AppData\Local\Temp\1588489035926.png)

​	不过，我们还应该再写一个调度器，将这几个模块组装起来。也就是说，先把输入模块、音频队列、视频队列都封装到音视频同步模块中，然后为外界提供获取音频数据、视频数据的接口，这两个接口必须保证音视频的同步，内部将负责解码线程的运行与暂停的维护。然后把音视频同步模块、音频输出模块、视频输出模块都封装到调度器中，调度器模块会分别向音频输出模块和视频输出模块注册回调函数，回调函数允许两个输出模块获取音频数据和视频数据。这样就可以对类图设计做进一步整理了，如下图所示。

![1588489084502](C:\Users\wjy\AppData\Local\Temp\1588489084502.png)

详细的解释具体如下。

- VideoPlayerController：调度器，内部维护音视频同步模块、音频输出模块、视频输出模块，为客户端代码提供开始播放、暂停、继续播放、停止播放接口；为音频输出模块和视频输出模块提供两个获取数据的接口
- AudioOutput：音频输出模块，由于在不同平台上有不同的实现，所以这里真正的声音渲染API为Void类型，但是音频的渲染要放在一个单独的线程（不论是平台API自动提供的线程，还是我们主动建立的线程）中进行，所以这里有一个线程的变量，在运行过程中会调用注册过来的回调函数来获取音频数据。
- VideoOutput：视频输出模块，虽然这里统一使用OpenGL ES来渲染视频，但是前文已提到过，OpenGLES的具体实现在不同的平台上也会有自己的上下文环境，所以这里采用了Void类型的实现，当然，必须由我们主动开启一个线程来作为OpenGL ES的渲染线程，它会在运行过程中调用注册过来的回调函数来获取视频数据
- AVSynchronizer：音视频同步模块，会组合后文将要讲到的输入模块、音频队列和视频队列，其主要为它的客户端代码VideoPlayerController调度器提供接口，包括：开始、结束，以及最重要的获取音频数据和获取对应时间戳的视频帧。此外，它还会维护一个解码线程，并且根据音视频队列里面的元素数目来继续或者暂停该解码线程的运行。
- AudioFrame：音频帧，其中记录了音频的数据格式以及这一帧的具体数据、时间戳等信息
- AudioFrameQueue：音频队列，主要用于存储音频帧，为它的客户端代码音视频同步模块提供压入和弹出操作，由于解码线程和声音播放线程会作为生产者和消费者同时访问该队列中的元素，所以该队列要保证线程安全性。
- VideoFrame：视频帧，记录了视频的格式以及这一帧的具体的数据、宽、高以及时间戳等信息。[插图]VideoFrameQueue：视频队列，主要用于存储视频帧，为它的客户端代码音视频同步模块提供压入和弹出操作，由于解码线程和视频播放线程会作为生产者和消费者同时访问该队列中的元素，所以该队列要保证线程安全性。
- VideoDecoder：输入模块，其职责在前面已经分析过了，由于还没有确定具体的技术实现，所以这里先根据前面的分析暂时写了三个实例变量，一个是协议层解析器，一个是格式解封装器，一个是解码器，并且它主要向AVSynchronizer提供接口：打开文件资源（网络或者本地）、关闭文件资源、解码出一定时间长度的音视频帧。

​	至此我们根据用户场景（Case）把视频播放器拆解成了各个模块，并且根据模块的调用关系画出了类图，那么接下来要做的事情就应该是拆分每个模块的具体实现。

​	首先是输入模块，如果是自己编写代码处理这些不同的协议、封装格式，以及编解码格式（更专业地来讲是各种解码器），肯定会是非常复杂也极其不合理的，因为这套东西已经非常成熟了，自行实现需要付出很大的开发与测试成本，并且最终效果也不会太理想。而基于我们现在所掌握的知识，可选择FFmpeg开源库的libavformat模块来处理各种不同的协议以及不同的封装格式。在解封装成每一路流之后，接下来就需要进行解码的操作，当然，最简单的也可以直接使用FFmpeg的libavcodec模块来进行，但是如果需要更高性能的解码，那么开发者可以使用Android和iOS平台各自的硬件解码器。本章暂时先不考虑优化，只是先快速实现出一套方案，使用软件解码会是一种比较好的选择，所以本章将使用FFmpeg的libavcodec模块作为解码器模块的技术选型。

​	其实对于架构来说，没有最好的设计，只有最合适的设计。在这里，硬件解码器对于系统平台是有限制的，同时还会有一些兼容性问题，并且两个平台还需要分别编写代码来实现各自的硬件解码器，并将硬件解码器解码出来的数据转换为可用于显示的视频帧数据结构。因为本章需要快速实现各个模块，所以这里选择使用软件解码器，同时它有更高的兼容性以及更简单的API调用。以后很可能需要通过硬件解码来提升性能，所以在设计解码模块的时候可以更多地使用面向接口的设计，以便日后更加方便地替换实现。

​	其次是音频输出模块，音频的输出其实有很多种方式，下面就来分别分析一下，首先是Android平台，最常用的就是Java层的AudioTrack和Native层的OpenSL ES。主要代码处于Native层，在AudioTrack和OpenSL ES之间，应该选择OpenSL ES，因为其省去了JNI的数据传递，并且OpenSL ES在播放声音方面的延迟更低，其缺点是所提供的API比起AudioTrack的不够友好，调试也不太方便，但是从总体来分析，还是选择OpenSL ES更合适；对于iOS平台，其实也有很多种方式，比较常见的就是AudioQueue和AudioUnit, AudioQueue是更高层次的音频API，是建立在AudioUnit的基础之上的，其所提供的API更加简单，在这里其实选用AudioQueue可能会更加合适，但是我们最终还是会选用AudioUnit，对此，有如下几个原因：首先可能存在音频格式的转换，这时AudioUnit会更加方便，并且这里还需要为后续的录音、音效处理打下使用AudioUnit的基础，所以这里将直接选择AudioUnit作为实现。

​	然后是视频输出模块，对此，技术选型肯定是选择OpenGL ES，因为我们可以利用它非常高效率的渲染视频，不论是在Android平台还是在iOS平台，前面也已经学习了如何在Android平台和iOS平台搭建OpenGL ES的环境。此外，在这里使用OpenGL ES还有一个好处，那就是我们可以利用OpenGL ES处理图像的巨大优势，来对视频做一个后期处理（通过去块滤波器、增加对比度等效果器的使用），让用户感觉视频更加清晰。在Android平台上使用EGL来为OpenGL ES提供上下文环境，使用SurfaceView的Surface来构造显示对象，并最终输出到SurfaceView上；在iOS平台上使用EAGL来为OpenGL ES提供上下文环境，自己定义一个View继承自UIView，使用EAGLLayer作为渲染对象，并最终渲染到这个自定义的View上。

​	至于音视频同步模块，这里不会涉及任何与平台相关的API，不过，考虑到它要维护解码线程，因此pthread其实是一个很好的选择，因为两个平台都支持这种线程模型。此外，它还需要维护两个队列，由于STL中提供的队列不能保证线程的安全性，所以对于音视频队列，我们可以自行编写一个保证线程安全的链表来实现。最后要负责音视频的同步，由于音视频同步的策略在前面的章节中已经提到过，因此这里采用视频向音频对齐的策略，即只需要把同步这块逻辑放到获取视频帧的方法里面就好了。

​	最后是控制器，控制器需要将上述的三个模块合理地组装起来。在开始播放的时候，需要把资源的地址（有可能是本地的文件，也有可能是网络的资源文件）传递给AVSynchronizer，如果能够成功地打开文件，那么就去实例化VideoOutput和AudioOutput，在实例化这两个类的同时，要传入回调函数，这两个回调函数又将分别调用AVSynchronizer里面的获取音频和视频帧的方法，这样就可以有序地组织多个模块，最终如果要调用暂停、继续或停止的指令，自然也就会调用各个模块对应的生命周期方法。

​	前文中笔者将每个模块的具体实现梳理了一遍，这样，其实架构已经基本成型了，但是对于架构师来说，做到这些还是不够的，因为优秀的架构师必须在做完整个架构之后，再针对该架构给出风险评估与部分测试用例，下面也将来逐一分析一下。

​	首先是风险评估，由于我们所做的最终项目是运行在移动平台上的，所以对于移动平台的碎片化设备（尤其是Android平台的碎片化更加严重）这一特点，必须要有足够多的设备作为测试目标，以保证没有兼容性方面的问题，设备所述的平台架构也应该覆盖到arm、armv7、arm64等平台。然后必须要测试性能问题，性能包括CPU消耗、内存占用、耗电量与发热量，而针对这些风险，在这一期项目中可能会有一些问题无法得到解决，因此我们的长期计划就应该在这些方面进行改进。其实，目前来说最大的风险就是软件解码这部分，因此从长期来看，需要有硬件解码的替代方案。

​	对于测试用例，我们应该从以下几个方面进行测试，首先是输入模块，包括协议层（网络资源、本地资源）、封装格式（FLV、MP4、MOV、AVI等）、编码格式（H264、AAC、WAV）等；其次是音视频同步模块，应该在低网速的条件下观看网络资源的对齐程度；最后是两个输出模块，测试应该要覆盖iOS系统和Android系统的大部分系统版本，以及最终应用运行的Top10的所有设备的音频和视频播放的兼容性。